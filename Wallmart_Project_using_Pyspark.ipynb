{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c50b9789-1c62-4019-8e70-399a11afee39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: pyspark\n",
      "Version: 4.0.0\n",
      "Summary: Apache Spark Python API\n",
      "Home-page: https://github.com/apache/spark/tree/master/python\n",
      "Author: Spark Developers\n",
      "Author-email: dev@spark.apache.org\n",
      "License: http://www.apache.org/licenses/LICENSE-2.0\n",
      "Location: C:\\Users\\ADIPAKSH\\AppData\\Roaming\\Python\\Python311\\site-packages\n",
      "Requires: py4j\n",
      "Required-by: \n"
     ]
    }
   ],
   "source": [
    "!pip show pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c1ebb4a8-cf0a-420e-8af8-5cbcb70977f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------------------+\n",
      "|Dept|  Total_Weekly_Sales|\n",
      "+----+--------------------+\n",
      "|   1|1.2363877654000005E8|\n",
      "|   2|2.8061117443000007E8|\n",
      "|   3| 7.589244995000002E7|\n",
      "|   4|1.6714674557999995E8|\n",
      "|   5|1.3560735857000008E8|\n",
      "|   6|2.8420667139999993E7|\n",
      "|   7|1.5547756275000027E8|\n",
      "|   8| 1.942807807300001E8|\n",
      "|   9|1.2839325665000004E8|\n",
      "|  10|1.1789738758000001E8|\n",
      "|  11| 9.332927637999998E7|\n",
      "|  12| 2.676011951000001E7|\n",
      "|  13|1.9732156995000008E8|\n",
      "|  14| 9.569466644999984E7|\n",
      "|  16| 9.167068226999989E7|\n",
      "|  17| 6.531981662999996E7|\n",
      "|  18| 3.689748906000001E7|\n",
      "|  19|   6816183.110000005|\n",
      "|  20| 3.334964510999999E7|\n",
      "|  21| 3.206464508000002E7|\n",
      "+----+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import sum, col\n",
    "from pyspark import SparkContext\n",
    "\n",
    "#10th code:-Department wise weekly sales(Aviral)\n",
    "\n",
    "# Step 1: Initialize SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"WeeklySalesByDepartment\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"4\") \\\n",
    "    .config(\"spark.executor.instances\", \"2\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Step 2: Initialize SparkContext\n",
    "sc = SparkContext.getOrCreate()\n",
    "spark.sparkContext.setLogLevel(\"INFO\")\n",
    "\n",
    "# Step 3: Load CSV as DataFrame\n",
    "df = spark.read.option(\"header\", True).option(\"inferSchema\", True).csv(\n",
    "    \"C:/Users/ADIPAKSH/Downloads/walmart-recruiting-store-sales-forecasting_source TEAM4/train.csv\"\n",
    ")\n",
    "\n",
    "# Step 4: Ensure correct data types\n",
    "df = df.withColumn(\"Weekly_Sales\", col(\"Weekly_Sales\").cast(\"double\"))\n",
    "\n",
    "# Step 5: Group by Department and calculate total weekly sales\n",
    "result_df = df.groupBy(\"Dept\") \\\n",
    "    .agg(sum(\"Weekly_Sales\").alias(\"Total_Weekly_Sales\")) \\\n",
    "    .orderBy(\"Dept\")\n",
    "\n",
    "# Step 6: Show result\n",
    "result_df.show()\n",
    "\n",
    "# Step 7: Save results in multiple formats\n",
    "result_df.write.mode(\"overwrite\").csv(\"C:/Users/ADIPAKSH/Desktop/output10-2/weekly_sales_by_department.csv\")\n",
    "result_df.write.mode(\"overwrite\").parquet(\"C:/Users/ADIPAKSH/Desktop/output10-2/weekly_sales_by_department_parquet\")\n",
    "result_df.write.mode(\"overwrite\").orc(\"C:/Users/ADIPAKSH/Desktop/output10-2/weekly_sales_by_department_orc\")\n",
    "\n",
    "# Step 8: Partition by Department\n",
    "result_df.write.partitionBy(\"Dept\").mode(\"overwrite\").parquet(\"C:/Users/ADIPAKSH/Desktop/output10-2/partitioned_by_department.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0e628d8c-1f09-4a7d-afa0-3ae74aa08ff6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------------------+\n",
      "|        CPI|  Total_Weekly_Sales|\n",
      "+-----------+--------------------+\n",
      "|    126.064|1.1186672609999998E7|\n",
      "|126.0766452|1.1236542210000005E7|\n",
      "|126.0854516|       1.161712406E7|\n",
      "|126.0892903|       1.064440361E7|\n",
      "|126.1019355|1.1446981889999995E7|\n",
      "|126.1069032|1.0543480790000001E7|\n",
      "|126.1119032|       1.147065679E7|\n",
      "|    126.114|1.1245237399999999E7|\n",
      "|126.1145806|1.1122414159999998E7|\n",
      "|   126.1266|1.0918034249999996E7|\n",
      "|126.1283548|       1.086616566E7|\n",
      "|126.1360645|1.1793210660000004E7|\n",
      "|   126.1392|1.1412042209999999E7|\n",
      "|126.1454667|1.0575165890000004E7|\n",
      "|126.1498065|1.1082464319999998E7|\n",
      "|   126.1518|1.0981588900000002E7|\n",
      "|126.1602258|1.0968939939999998E7|\n",
      "|126.1843871|       1.092779466E7|\n",
      "|126.1900333|1.0191666909999998E7|\n",
      "|126.2085484|       1.106177064E7|\n",
      "+-----------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#9th code:- Weekly sales by CPI(Aviral)\n",
    "# Step 1: Initialize SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"WeeklySalesByCPI\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"4\") \\\n",
    "    .config(\"spark.executor.instances\", \"2\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "spark.sparkContext.setLogLevel(\"INFO\")\n",
    "\n",
    "# Step 2: Load train.csv as DataFrame\n",
    "train_df = spark.read.option(\"header\", True).option(\"inferSchema\", True).csv(\n",
    "    \"C:/Users/ADIPAKSH/Downloads/walmart-recruiting-store-sales-forecasting_source TEAM4/train.csv\"\n",
    ")\n",
    "\n",
    "# Step 3: Load features.csv to get CPI\n",
    "features_df = spark.read.option(\"header\", True).option(\"inferSchema\", True).csv(\n",
    "    \"C:/Users/ADIPAKSH/Downloads/walmart-recruiting-store-sales-forecasting_source TEAM4/features.csv\"\n",
    ")\n",
    "\n",
    "# Step 4: Join train data with CPI\n",
    "joined_df = train_df.join(features_df.select(\"Store\", \"Date\", \"CPI\"), on=[\"Store\", \"Date\"])\n",
    "\n",
    "# Step 5: Ensure Weekly_Sales is cast to double\n",
    "joined_df = joined_df.withColumn(\"Weekly_Sales\", col(\"Weekly_Sales\").cast(\"double\"))\n",
    "\n",
    "# Step 6: Group by CPI and calculate total weekly sales\n",
    "result_df = joined_df.groupBy(\"CPI\") \\\n",
    "    .agg(sum(\"Weekly_Sales\").alias(\"Total_Weekly_Sales\")) \\\n",
    "    .orderBy(\"CPI\")\n",
    "\n",
    "# Step 7: Show result\n",
    "result_df.show()\n",
    "\n",
    "# Step 8: Write output to local file system\n",
    "result_df.write.mode(\"overwrite\").csv(\"C:/Users/ADIPAKSH/Desktop/output10-1/weekly_sales_by_cpi.csv\")\n",
    "\n",
    "# Step 9: Write as Parquet and ORC\n",
    "result_df.write.mode(\"overwrite\").parquet(\"C:/Users/ADIPAKSH/Desktop/output10-1/weekly_sales_by_cpi_parquet\")\n",
    "result_df.write.mode(\"overwrite\").orc(\"C:/Users/ADIPAKSH/Desktop/output10-1/weekly_sales_by_cpi_orc\")\n",
    "\n",
    "# Step 10: Partition by CPI\n",
    "result_df.write.partitionBy(\"CPI\").mode(\"overwrite\").parquet(\"C:/Users/ADIPAKSH/Desktop/output10-1/partitioned_by_cpi\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4ac802b8-79b9-4edb-8ea9-094572503935",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---------+--------------------+\n",
      "|Store|IsHoliday|  Total_Weekly_Sales|\n",
      "+-----+---------+--------------------+\n",
      "|    1|    false|2.0574533228999949E8|\n",
      "|    1|     true|1.6657476559999997E7|\n",
      "|    2|    false|2.5458977198000062E8|\n",
      "|    2|     true|2.0792669000000022E7|\n",
      "|    3|    false| 5.320862456999985E7|\n",
      "|    3|     true|           4378110.5|\n",
      "|    4|    false| 2.771129271399988E8|\n",
      "|    4|     true|2.2431026240000024E7|\n",
      "|    5|    false|  4.18806728300003E7|\n",
      "|    5|     true|   3595016.070000001|\n",
      "|    6|    false|2.0694705137000006E8|\n",
      "|    6|     true|1.6809079270000003E7|\n",
      "|    7|    false|       7.487427249E7|\n",
      "|    7|     true|   6724002.650000009|\n",
      "|    8|    false|1.2019787252999978E8|\n",
      "|    8|     true|   9753308.600000001|\n",
      "|    9|    false| 7.189971077999982E7|\n",
      "|    9|     true|   5889508.209999993|\n",
      "|   10|    false| 2.504801543999997E8|\n",
      "|   10|     true|2.1137559489999976E7|\n",
      "+-----+---------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Code 1:- Weekly sales by Store and Holiday(Anagha)\n",
    "from pyspark.sql.functions import sum, col, avg\n",
    "\n",
    "# Step 1: Initialize SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"WeeklySalesByStoreHoliday\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"4\") \\\n",
    "    .config(\"spark.executor.instances\", \"2\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "spark.sparkContext.setLogLevel(\"INFO\")\n",
    "\n",
    "# Step 2: Load CSV as RDD\n",
    "rdd = spark.sparkContext.textFile(\"C:/Users/ADIPAKSH/Downloads/walmart-recruiting-store-sales-forecasting_source TEAM4/train.csv\")\n",
    "\n",
    "# Step 3: Convert RDD to DataFrame\n",
    "header = rdd.first()\n",
    "data_rdd = rdd.filter(lambda row: row != header).map(lambda row: row.split(\",\"))\n",
    "df = data_rdd.map(lambda arr: (int(arr[0]), int(arr[1]), arr[2], float(arr[3]), arr[4] == 'TRUE')) \\\n",
    "    .toDF([\"Store\", \"Dept\", \"Date\", \"Weekly_Sales\", \"IsHoliday\"])\n",
    "\n",
    "# Step 4: Group by Store and IsHoliday\n",
    "result_df = df.groupBy(\"Store\", \"IsHoliday\") \\\n",
    "    .agg(sum(\"Weekly_Sales\").alias(\"Total_Weekly_Sales\")) \\\n",
    "    .orderBy(\"Store\", \"IsHoliday\")\n",
    "\n",
    "# Step 5: Show result\n",
    "result_df.show()\n",
    "\n",
    "# Step 6: Write output to local file system\n",
    "result_df.write.mode(\"overwrite\").csv(\"C:/Users/ADIPAKSH/Desktop/output1-1/weekly_sales_by_store_holiday.csv\")\n",
    "\n",
    "# Step 7: Write as Parquet and ORC\n",
    "result_df.write.mode(\"overwrite\").parquet(\"C:/Users/ADIPAKSH/Desktop/output1-1/weekly_sales_by_store_holiday_parquet\")\n",
    "result_df.write.mode(\"overwrite\").orc(\"C:/Users/ADIPAKSH/Desktop/output1-1/weekly_sales_by_store_holiday_orc\")\n",
    "\n",
    "# Step 8: Partition DF by IsHoliday\n",
    "result_df.write.partitionBy(\"IsHoliday\").mode(\"overwrite\").parquet(\"C:/Users/ADIPAKSH/Desktop/output1-1/partitioned_by_holiday\")\n",
    "\n",
    "\n",
    "# Optional: Stop Spark session\n",
    "# spark.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "83116091-1c3f-4863-a3fc-52fd0ee41c6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----+------------------+\n",
      "|Temperature|Year|Total_Weekly_Sales|\n",
      "+-----------+----+------------------+\n",
      "|       6.23|2011|1083071.1399999994|\n",
      "|       9.55|2010|1034119.2099999997|\n",
      "|      10.91|2011|1083657.6099999999|\n",
      "|      11.17|2011|1565652.4700000002|\n",
      "|      12.19|2011|1059715.2700000003|\n",
      "|      14.48|2010|1001943.8000000002|\n",
      "|       14.5|2011|        1159438.53|\n",
      "|      14.56|2011|1179420.5000000005|\n",
      "|      15.12|2011|1010711.0800000001|\n",
      "|      15.25|2010|1364721.5799999998|\n",
      "|      15.33|2012|        1146992.13|\n",
      "|      15.47|2011|1070457.7999999996|\n",
      "|      15.58|2011|        1110706.06|\n",
      "|       16.6|2010|1149612.0399999998|\n",
      "|      16.81|2011|1150003.3600000003|\n",
      "|      16.94|2011|        2032864.77|\n",
      "|       17.0|2011|         1206917.2|\n",
      "|      17.05|2011|1548661.4500000004|\n",
      "|      17.46|2011|1017056.4300000002|\n",
      "|      17.91|2011|        1126921.34|\n",
      "+-----------+----+------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "Count of filtered rows: 2852\n"
     ]
    }
   ],
   "source": [
    "#Code 2:- weekly sales by temperature and year(Anagha)\n",
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import sum, col, year, to_date\n",
    "\n",
    "# Step 1: Initialize SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"WeeklySalesByTempYear\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"4\") \\\n",
    "    .config(\"spark.executor.instances\", \"2\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "spark.sparkContext.setLogLevel(\"INFO\")\n",
    "\n",
    "# Step 2: Load train.csv as RDD\n",
    "rdd = spark.sparkContext.textFile(\"C:/Users/ADIPAKSH/Downloads/walmart-recruiting-store-sales-forecasting_source TEAM4/train.csv\")\n",
    "\n",
    "# Step 3: Convert RDD to DataFrame\n",
    "header = rdd.first()\n",
    "data_rdd = rdd.filter(lambda row: row != header).map(lambda row: row.split(\",\"))\n",
    "df = data_rdd.map(lambda arr: (int(arr[0]), int(arr[1]), arr[2], float(arr[3]), arr[4] == 'TRUE')) \\\n",
    "    .toDF([\"Store\", \"Dept\", \"Date\", \"Weekly_Sales\", \"IsHoliday\"])\n",
    "\n",
    "# Step 4: Load features.csv and join\n",
    "features_df = spark.read.option(\"header\", True).option(\"inferSchema\", True).csv(\n",
    "    \"C:/Users/ADIPAKSH/Downloads/walmart-recruiting-store-sales-forecasting_source TEAM4/features.csv\"\n",
    ")\n",
    "\n",
    "joined_df = df.join(features_df, on=[\"Store\", \"Date\", \"IsHoliday\"], how=\"left\")\n",
    "\n",
    "# Step 5: Extract year and perform analysis\n",
    "with_year_df = joined_df.withColumn(\"Year\", year(to_date(col(\"Date\"), \"yyyy-MM-dd\")))\n",
    "result_df = with_year_df.groupBy(\"Temperature\", \"Year\") \\\n",
    "    .agg(sum(\"Weekly_Sales\").alias(\"Total_Weekly_Sales\")) \\\n",
    "    .orderBy(\"Temperature\", \"Year\")\n",
    "\n",
    "# Step 6: Write output to local file system\n",
    "result_df.write.mode(\"overwrite\").csv(\"C:/Users/ADIPAKSH/Desktop/Pyspark/output2-1/weekly_sales_by_temp_year.csv\")\n",
    "\n",
    "# Step 7: Write as Parquet and ORC\n",
    "result_df.write.mode(\"overwrite\").parquet(\"C:/Users/ADIPAKSH/Desktop/Pyspark/output2-1/weekly_sales_parquet\")\n",
    "result_df.write.mode(\"overwrite\").orc(\"C:/Users/ADIPAKSH/Desktop/Pyspark/output2-1/weekly_sales_orc\")\n",
    "\n",
    "# Step 8: Partition DF by Year\n",
    "result_df.write.partitionBy(\"Year\").mode(\"overwrite\").parquet(\"C:/Users/ADIPAKSH/Desktop/Pyspark/output2-1/partitioned_by_year\")\n",
    "\n",
    "# Step 9: Transformations and actions\n",
    "filtered_df = result_df.filter(col(\"Total_Weekly_Sales\") > 1000000)\n",
    "filtered_df.show()\n",
    "print(f\"Count of filtered rows: {filtered_df.count()}\")\n",
    "\n",
    "\n",
    "# Optional: Stop Spark session\n",
    "# spark.stop()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c79f65c8-9d97-4ffc-9a13-65d3b68314c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------------------+\n",
      "|  Size|  Total_Weekly_Sales|\n",
      "+------+--------------------+\n",
      "| 34875|4.5475688900000155E7|\n",
      "| 37392| 5.758673506999983E7|\n",
      "| 39690|1.7188560080999953E8|\n",
      "| 39910|1.7090804312999853E8|\n",
      "| 41062| 9.056543541000025E7|\n",
      "| 42988| 6.271688512000011E7|\n",
      "| 57197| 7.425242539999999E7|\n",
      "| 70713| 8.159827513999955E7|\n",
      "| 93188|1.2778213882999982E8|\n",
      "| 93638| 7.714155430999991E7|\n",
      "|103681|1.3152067207999997E8|\n",
      "|112238| 1.442872301499992E8|\n",
      "|114533|1.9875061784999925E8|\n",
      "|118221| 1.123953414199994E8|\n",
      "|119557|1.4707564857000002E8|\n",
      "|120653|1.5511473421000072E8|\n",
      "|123737| 8.913368392000023E7|\n",
      "|125833| 7.778921898999973E7|\n",
      "|126512| 2.716177138899999E8|\n",
      "|128107| 1.010611791700001E8|\n",
      "+------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Code 3:- Weekly Sales By Stores Size(Gauri)\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import sum, col\n",
    "\n",
    "# Step 1: Initialize SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"WeeklySalesByStoreSize\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"4\") \\\n",
    "    .config(\"spark.executor.instances\", \"2\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "spark.sparkContext.setLogLevel(\"INFO\")\n",
    "\n",
    "# Step 2: Load train.csv as RDD\n",
    "rdd = spark.sparkContext.textFile(\"C:/Users/ADIPAKSH/Downloads/walmart-recruiting-store-sales-forecasting_source TEAM4/train.csv\")\n",
    "\n",
    "# Step 3: Convert RDD to DataFrame\n",
    "header = rdd.first()\n",
    "data_rdd = rdd.filter(lambda row: row != header).map(lambda row: row.split(\",\"))\n",
    "df = data_rdd.map(lambda arr: (int(arr[0]), int(arr[1]), arr[2], float(arr[3]), arr[4] == 'TRUE')) \\\n",
    "    .toDF([\"Store\", \"Dept\", \"Date\", \"Weekly_Sales\", \"IsHoliday\"])\n",
    "\n",
    "# Step 4: Load stores.csv to get Store Size\n",
    "stores_df = spark.read.option(\"header\", True).option(\"inferSchema\", True).csv(\n",
    "    \"C:/Users/ADIPAKSH/Downloads/walmart-recruiting-store-sales-forecasting_source TEAM4/stores.csv\"\n",
    ")\n",
    "\n",
    "# Step 5: Join train data with store size\n",
    "joined_df = df.join(stores_df.select(\"Store\", \"Size\"), on=\"Store\")\n",
    "\n",
    "# Step 6: Group by Store Size and calculate total weekly sales\n",
    "result_df = joined_df.groupBy(\"Size\") \\\n",
    "    .agg(sum(\"Weekly_Sales\").alias(\"Total_Weekly_Sales\")) \\\n",
    "    .orderBy(\"Size\")\n",
    "\n",
    "# Step 7: Show result\n",
    "result_df.show()\n",
    "\n",
    "# Step 8: Write output to local file system\n",
    "result_df.write.mode(\"overwrite\").csv(\"C:/Users/ADIPAKSH/Desktop/Pyspark/output3-1/weekly_sales_by_store_size.csv\")\n",
    "\n",
    "# Step 9: Write as Parquet and ORC\n",
    "result_df.write.mode(\"overwrite\").parquet(\"C:/Users/ADIPAKSH/Desktop/Pyspark/output3-1/weekly_sales_by_store_size_parquet\")\n",
    "result_df.write.mode(\"overwrite\").orc(\"C:/Users/ADIPAKSH/Desktop/Pyspark/output3-1/weekly_sales_by_store_size_orc\")\n",
    "\n",
    "# Step 10: Partition by Size\n",
    "result_df.write.partitionBy(\"Size\").mode(\"overwrite\").parquet(\"C:/Users/ADIPAKSH/Desktop/Pyspark/output3-1/partitioned_by_size\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "30f9dd8e-d42d-49ce-a535-7f202bc08158",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+--------------------+\n",
      "|Type|Month|  Total_Weekly_Sales|\n",
      "+----+-----+--------------------+\n",
      "|   A|    1|2.1417616809999934E8|\n",
      "|   A|    2| 3.665076723800007E8|\n",
      "|   A|    3|3.8077453309999824E8|\n",
      "|   A|    4| 4.161801298799987E8|\n",
      "|   A|    5| 3.590865958399991E8|\n",
      "|   A|    6| 3.994480057699998E8|\n",
      "|   A|    7| 4.172432103900018E8|\n",
      "|   A|    8|3.9486368368999904E8|\n",
      "|   A|    9| 3.731186221900001E8|\n",
      "|   A|   10| 3.771314802799996E8|\n",
      "|   A|   11|2.6472138660999936E8|\n",
      "|   A|   12| 3.677632345200002E8|\n",
      "|   B|    1| 9.544645479999976E7|\n",
      "|   B|    2| 1.676717237399994E8|\n",
      "|   B|    3|1.7513603761000013E8|\n",
      "|   B|    4|1.9088059458999997E8|\n",
      "|   B|    5|1.6345560924999985E8|\n",
      "|   B|    6|1.8636250035999995E8|\n",
      "|   B|    7|1.9374323200000003E8|\n",
      "|   B|    8| 1.815049900400005E8|\n",
      "+----+-----+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Code 4:- Weekly Sales BY Store Type And Month(Gauri)\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import sum, col, month, to_date\n",
    "\n",
    "# Step 1: Initialize SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"WeeklySalesByStoreTypeMonth\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"4\") \\\n",
    "    .config(\"spark.executor.instances\", \"2\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "spark.sparkContext.setLogLevel(\"INFO\")\n",
    "\n",
    "# Step 2: Load train.csv as RDD\n",
    "rdd = spark.sparkContext.textFile(\"C:/Users/ADIPAKSH/Downloads/walmart-recruiting-store-sales-forecasting_source TEAM4/train.csv\")\n",
    "\n",
    "# Step 3: Convert RDD to DataFrame\n",
    "header = rdd.first()\n",
    "data_rdd = rdd.filter(lambda row: row != header).map(lambda row: row.split(\",\"))\n",
    "df = data_rdd.map(lambda arr: (int(arr[0]), int(arr[1]), arr[2], float(arr[3]), arr[4] == 'TRUE')) \\\n",
    "    .toDF([\"Store\", \"Dept\", \"Date\", \"Weekly_Sales\", \"IsHoliday\"])\n",
    "\n",
    "# Step 4: Load stores.csv to get Store Type\n",
    "stores_df = spark.read.option(\"header\", True).option(\"inferSchema\", True).csv(\n",
    "    \"C:/Users/ADIPAKSH/Downloads/walmart-recruiting-store-sales-forecasting_source TEAM4/stores.csv\"\n",
    ")\n",
    "\n",
    "# Step 5: Join train data with store type\n",
    "joined_df = df.join(stores_df.select(\"Store\", \"Type\"), on=\"Store\")\n",
    "\n",
    "# Step 6: Extract month from Date\n",
    "with_month_df = joined_df.withColumn(\"Month\", month(to_date(col(\"Date\"), \"yyyy-MM-dd\")))\n",
    "\n",
    "# Step 7: Group by Store Type and Month\n",
    "result_df = with_month_df.groupBy(\"Type\", \"Month\") \\\n",
    "    .agg(sum(\"Weekly_Sales\").alias(\"Total_Weekly_Sales\")) \\\n",
    "    .orderBy(\"Type\", \"Month\")\n",
    "\n",
    "# Step 8: Show result\n",
    "result_df.show()\n",
    "\n",
    "# Step 9: Write output to local file system\n",
    "result_df.write.mode(\"overwrite\").csv(\"C:/Users/ADIPAKSH/Desktop/Pyspark/output4-1/weekly_sales_by_store_type_month.csv\")\n",
    "\n",
    "# Step 10: Write as Parquet and ORC\n",
    "result_df.write.mode(\"overwrite\").parquet(\"C:/Users/ADIPAKSH/Desktop/Pyspark/output4-1/weekly_sales_by_store_type_month_parquet\")\n",
    "result_df.write.mode(\"overwrite\").orc(\"C:/Users/ADIPAKSH/Desktop/Pyspark/output4-1/weekly_sales_by_store_type_month_orc\")\n",
    "\n",
    "# Step 11: Partition by Type\n",
    "result_df.write.partitionBy(\"Type\").mode(\"overwrite\").parquet(\"C:/Users/ADIPAKSH/Desktop/Pyspark/output4-1/partitioned_by_type\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d25adf4f-c13f-46ef-b771-e18053e1eaac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----+------------------+------------------+------------------+------------------+------------------+\n",
      "|Store|Year|   Total_MarkDown1|   Total_MarkDown2|   Total_MarkDown3|   Total_MarkDown4|   Total_MarkDown5|\n",
      "+-----+----+------------------+------------------+------------------+------------------+------------------+\n",
      "|    1|2011|40636.270000000004|          52673.92|          58818.12|           9800.41|          59518.63|\n",
      "|    1|2012|345161.57999999996|         116615.72|          76716.44|         171364.42|171109.61000000002|\n",
      "|    1|2013|261880.06999999998| 74836.09999999999|           6701.42|         112954.32| 93071.72000000002|\n",
      "|    2|2011|53893.159999999996|         114448.38|          82233.17|16329.399999999998| 65380.59000000001|\n",
      "|    2|2012| 463558.3599999999|173349.87000000005|107864.51999999999|208356.49000000002|233636.18000000008|\n",
      "|    2|2013|         280722.32| 83392.78000000001| 7583.800000000001|132646.15999999997|142664.64000000004|\n",
      "|    3|2011|          16530.52|          16411.37|1191.7500000000002|           2744.82|          15023.91|\n",
      "|    3|2012|104863.64000000001|31237.240000000005|489.09000000000003|          31625.54|29933.160000000007|\n",
      "|    3|2013|          82397.38|18155.370000000003|1893.8199999999997|16656.619999999995|          23438.21|\n",
      "|    4|2011|42287.759999999995|          90382.91|          92516.79|          17354.28|          85590.18|\n",
      "|    4|2012|411918.02999999997|170718.86000000002|          96565.19|         211491.62|213690.12999999998|\n",
      "|    4|2013|         346056.19| 73178.05999999998|           8627.18|186539.74000000002|128276.88000000002|\n",
      "|    5|2011|           8328.83|          19150.07|          30219.04|           2124.46|32199.350000000002|\n",
      "|    5|2012|109220.46999999997|33278.520000000004|            265.07|          46664.81|64210.000000000015|\n",
      "|    5|2013|           66550.3| 8759.040000000003|            1423.3|          26822.29|          29419.83|\n",
      "|    6|2011|44718.840000000004| 85730.54000000001| 89036.83999999998|          13499.31|          65790.56|\n",
      "|    6|2012| 377559.1099999999|          160293.2|2906.5000000000005|169204.88000000003|         199955.65|\n",
      "|    6|2013| 294535.0599999999|130272.59000000003|           5931.26|122259.99999999999|122165.65000000001|\n",
      "|    7|2011|          13430.25|          19696.09|          43141.44|4093.7500000000005|          22460.56|\n",
      "|    7|2012|         253279.23| 56079.19999999999| 576.5300000000001| 73966.77999999998|130117.81000000001|\n",
      "+-----+----+------------------+------------------+------------------+------------------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#code 5:- Markdown Sales by year and store(Ayushi)\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import sum, col, year, to_date\n",
    "from pyspark.sql.types import DoubleType\n",
    "\n",
    "# Step 1: Initialize SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"MarkdownSalesByYearStore\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"4\") \\\n",
    "    .config(\"spark.executor.instances\", \"2\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "spark.sparkContext.setLogLevel(\"INFO\")\n",
    "\n",
    "# Step 2: Load features.csv\n",
    "features_df = spark.read.option(\"header\", True).option(\"inferSchema\", True).csv(\n",
    "    \"C:/Users/ADIPAKSH/Downloads/walmart-recruiting-store-sales-forecasting_source TEAM4/features.csv\"\n",
    ")\n",
    "\n",
    "# Step 3: Cast Markdown columns to DoubleType\n",
    "for col_name in [\"MarkDown1\", \"MarkDown2\", \"MarkDown3\", \"MarkDown4\", \"MarkDown5\"]:\n",
    "    features_df = features_df.withColumn(col_name, col(col_name).cast(DoubleType()))\n",
    "\n",
    "# Step 4: Drop rows with nulls or NaNs in critical columns\n",
    "cleaned_df = features_df.dropna(subset=[\n",
    "    \"Store\", \"Date\", \"MarkDown1\", \"MarkDown2\", \"MarkDown3\", \"MarkDown4\", \"MarkDown5\"\n",
    "])\n",
    "\n",
    "# Step 5: Extract year from Date\n",
    "with_year_df = cleaned_df.withColumn(\"Year\", year(to_date(col(\"Date\"), \"yyyy-MM-dd\")))\n",
    "\n",
    "# Step 6: Group and aggregate\n",
    "markdown_df = with_year_df.groupBy(\"Store\", \"Year\").agg(\n",
    "    sum(\"MarkDown1\").alias(\"Total_MarkDown1\"),\n",
    "    sum(\"MarkDown2\").alias(\"Total_MarkDown2\"),\n",
    "    sum(\"MarkDown3\").alias(\"Total_MarkDown3\"),\n",
    "    sum(\"MarkDown4\").alias(\"Total_MarkDown4\"),\n",
    "    sum(\"MarkDown5\").alias(\"Total_MarkDown5\")\n",
    ").orderBy(\"Store\", \"Year\")\n",
    "\n",
    "# Step 7: Show result\n",
    "markdown_df.show()\n",
    "\n",
    "# Step 8: Save outputs\n",
    "markdown_df.write.mode(\"overwrite\").csv(\"C:/Users/ADIPAKSH/Desktop/Pyspark/output5/markdown_sales_by_year_store.csv\")\n",
    "markdown_df.write.mode(\"overwrite\").parquet(\"C:/Users/ADIPAKSH/Desktop/Pyspark/output5/markdown_sales_by_year_store_parquet\")\n",
    "markdown_df.write.mode(\"overwrite\").orc(\"C:/Users/ADIPAKSH/Desktop/Pyspark/output5/markdown_sales_by_year_store_orc\")\n",
    "markdown_df.write.partitionBy(\"Year\").mode(\"overwrite\").parquet(\"C:/Users/ADIPAKSH/Desktop/Pyspark/output5/partitioned_by_year_markdown\")\n",
    "\n",
    "# Optional: Stop Spark session\n",
    "# spark.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a46ee5fe-eb77-4e74-a5f9-2b2c31704757",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------------------+\n",
      "|Type|  Total_Weekly_Sales|\n",
      "+----+--------------------+\n",
      "|   A|4.3310147227500725E9|\n",
      "|   B|2.0007007368200192E9|\n",
      "|   C| 4.055035275399986E8|\n",
      "+----+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Code 6:- Weekly Sales By Store Type(Ayushi)\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import sum, col\n",
    "\n",
    "# Step 1: Initialize SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"WeeklySalesByStoreType\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"4\") \\\n",
    "    .config(\"spark.executor.instances\", \"2\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "spark.sparkContext.setLogLevel(\"INFO\")\n",
    "\n",
    "# Step 2: Load train.csv as RDD\n",
    "rdd = spark.sparkContext.textFile(\"C:/Users/ADIPAKSH/Downloads/walmart-recruiting-store-sales-forecasting_source TEAM4/train.csv\")\n",
    "\n",
    "# Step 3: Convert RDD to DataFrame\n",
    "header = rdd.first()\n",
    "data_rdd = rdd.filter(lambda row: row != header).map(lambda row: row.split(\",\"))\n",
    "df = data_rdd.map(lambda arr: (int(arr[0]), int(arr[1]), arr[2], float(arr[3]), arr[4] == 'TRUE')) \\\n",
    "    .toDF([\"Store\", \"Dept\", \"Date\", \"Weekly_Sales\", \"IsHoliday\"])\n",
    "\n",
    "# Step 4: Load stores.csv to get Store Type\n",
    "stores_df = spark.read.option(\"header\", True).option(\"inferSchema\", True).csv(\n",
    "    \"C:/Users/ADIPAKSH/Downloads/walmart-recruiting-store-sales-forecasting_source TEAM4/stores.csv\"\n",
    ")\n",
    "\n",
    "# Step 5: Join train data with store type\n",
    "joined_df = df.join(stores_df.select(\"Store\", \"Type\"), on=\"Store\")\n",
    "\n",
    "# Step 6: Group by Store Type and calculate total weekly sales\n",
    "result_df = joined_df.groupBy(\"Type\") \\\n",
    "    .agg(sum(\"Weekly_Sales\").alias(\"Total_Weekly_Sales\")) \\\n",
    "    .orderBy(\"Type\")\n",
    "\n",
    "# Step 7: Show result\n",
    "result_df.show()\n",
    "\n",
    "# Step 8: Write output to local file system\n",
    "result_df.write.mode(\"overwrite\").csv(\"C:/Users/ADIPAKSH/Desktop/Pyspark/output6/weekly_sales_by_store_type.csv\")\n",
    "\n",
    "# Step 9: Write as Parquet and ORC\n",
    "result_df.write.mode(\"overwrite\").parquet(\"C:/Users/ADIPAKSH/Desktop/Pyspark/output6/weekly_sales_by_store_type_parquet\")\n",
    "result_df.write.mode(\"overwrite\").orc(\"C:/Users/ADIPAKSH/Desktop/Pyspark/output6/weekly_sales_by_store_type_orc\")\n",
    "\n",
    "# Step 10: Partition by Type\n",
    "result_df.write.partitionBy(\"Type\").mode(\"overwrite\").parquet(\"C:/Users/ADIPAKSH/Desktop/Pyspark/output6/partitioned_by_type\")\n",
    "\n",
    "\n",
    "# Optional: Stop Spark session\n",
    "# spark.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "924b4b95-ffb5-4f83-afc5-389fd38fbe1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+------------------+\n",
      "|Year|Average_Fuel_Price|\n",
      "+----+------------------+\n",
      "|2010|2.8237671296296383|\n",
      "|2011| 3.561914957264957|\n",
      "|2012| 3.672084188034199|\n",
      "|2013| 3.606057777777775|\n",
      "+----+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Code 7:- Fuel Price By Year(Sanchit)\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import year, to_date, col, avg\n",
    "\n",
    "# Step 1: Initialize SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"FuelPriceByYear\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"4\") \\\n",
    "    .config(\"spark.executor.instances\", \"2\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "spark.sparkContext.setLogLevel(\"INFO\")\n",
    "\n",
    "# Step 2: Load features.csv\n",
    "features_df = spark.read.option(\"header\", True).option(\"inferSchema\", True).csv(\n",
    "    \"C:/Users/ADIPAKSH/Downloads/walmart-recruiting-store-sales-forecasting_source TEAM4/features.csv\"\n",
    ")\n",
    "\n",
    "# Step 3: Extract year from Date\n",
    "with_year_df = features_df.withColumn(\"Year\", year(to_date(col(\"Date\"), \"yyyy-MM-dd\")))\n",
    "\n",
    "# Step 4: Group by Year and calculate average fuel price\n",
    "result_df = with_year_df.groupBy(\"Year\") \\\n",
    "    .agg(avg(\"Fuel_Price\").alias(\"Average_Fuel_Price\")) \\\n",
    "    .orderBy(\"Year\")\n",
    "\n",
    "# Step 5: Show result\n",
    "result_df.show()\n",
    "\n",
    "# Step 6: Write output to local file system\n",
    "result_df.write.mode(\"overwrite\").csv(\"C:/Users/ADIPAKSH/Desktop/Pyspark/output7/fuel_price_by_year.csv\")\n",
    "\n",
    "# Step 7: Write as Parquet and ORC\n",
    "result_df.write.mode(\"overwrite\").parquet(\"C:/Users/ADIPAKSH/Desktop/Pyspark/output7/fuel_price_by_year_parquet\")\n",
    "result_df.write.mode(\"overwrite\").orc(\"C:/Users/ADIPAKSH/Desktop/Pyspark/output7/fuel_price_by_year_orc\")\n",
    "\n",
    "# Step 8: Partition by Year\n",
    "result_df.write.partitionBy(\"Year\").mode(\"overwrite\").parquet(\"C:/Users/ADIPAKSH/Desktop/Pyspark/output7/partitioned_by_year_fuel\")\n",
    "\n",
    "# Optional: Stop Spark session\n",
    "# spark.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "b6416f09-cbe6-4878-8a85-3d3ca0d37717",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+----------+--------------------+\n",
      "|Year|Month|      Date|  Total_Weekly_Sales|\n",
      "+----+-----+----------+--------------------+\n",
      "|2010|    2|2010-02-05|4.9750740500000045E7|\n",
      "|2010|    2|2010-02-12| 4.833667763000007E7|\n",
      "|2010|    2|2010-02-19| 4.827699377999997E7|\n",
      "|2010|    2|2010-02-26|       4.396857113E7|\n",
      "|2010|    3|2010-03-05| 4.687147029999999E7|\n",
      "|2010|    3|2010-03-12| 4.592539651000002E7|\n",
      "|2010|    3|2010-03-19|       4.498897464E7|\n",
      "|2010|    3|2010-03-26| 4.413396105000002E7|\n",
      "|2010|    4|2010-04-02|5.0423831259999976E7|\n",
      "|2010|    4|2010-04-09|       4.736529044E7|\n",
      "|2010|    4|2010-04-16|4.5183667079999976E7|\n",
      "|2010|    4|2010-04-23| 4.473445256000005E7|\n",
      "|2010|    4|2010-04-30| 4.370512670999997E7|\n",
      "|2010|    5|2010-05-07|       4.850324352E7|\n",
      "|2010|    5|2010-05-14| 4.533008019999998E7|\n",
      "|2010|    5|2010-05-21| 4.512010805999997E7|\n",
      "|2010|    5|2010-05-28| 4.775750255999994E7|\n",
      "|2010|    6|2010-06-04| 5.018854312000005E7|\n",
      "|2010|    6|2010-06-11| 4.782654671999997E7|\n",
      "|2010|    6|2010-06-18| 4.762204623000002E7|\n",
      "+----+-----+----------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Code 8:- Weekly sales by year, month and date(Sanchit)\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import to_date, year, month, dayofmonth, col, sum\n",
    "\n",
    "# Step 1: Initialize SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"WeeklySalesByYearMonthDate\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"4\") \\\n",
    "    .config(\"spark.executor.instances\", \"2\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "spark.sparkContext.setLogLevel(\"INFO\")\n",
    "\n",
    "# Step 2: Load train.csv as RDD\n",
    "rdd = spark.sparkContext.textFile(\"C:/Users/ADIPAKSH/Downloads/walmart-recruiting-store-sales-forecasting_source TEAM4/train.csv\")\n",
    "\n",
    "# Step 3: Convert RDD to DataFrame\n",
    "header = rdd.first()\n",
    "data_rdd = rdd.filter(lambda row: row != header).map(lambda row: row.split(\",\"))\n",
    "df = data_rdd.map(lambda arr: (int(arr[0]), int(arr[1]), arr[2], float(arr[3]), arr[4] == 'TRUE')) \\\n",
    "    .toDF([\"Store\", \"Dept\", \"Date\", \"Weekly_Sales\", \"IsHoliday\"])\n",
    "\n",
    "# Step 4: Extract Year, Month, and Day\n",
    "with_date_df = df.withColumn(\"ParsedDate\", to_date(col(\"Date\"), \"yyyy-MM-dd\")) \\\n",
    "    .withColumn(\"Year\", year(col(\"ParsedDate\"))) \\\n",
    "    .withColumn(\"Month\", month(col(\"ParsedDate\"))) \\\n",
    "    .withColumn(\"Day\", dayofmonth(col(\"ParsedDate\")))\n",
    "\n",
    "# Step 5: Group by Year, Month, and Date\n",
    "result_df = with_date_df.groupBy(\"Year\", \"Month\", \"Date\") \\\n",
    "    .agg(sum(\"Weekly_Sales\").alias(\"Total_Weekly_Sales\")) \\\n",
    "    .orderBy(\"Year\", \"Month\", \"Date\")\n",
    "\n",
    "# Step 6: Show result\n",
    "result_df.show()\n",
    "\n",
    "# Step 7: Write output to local file system\n",
    "result_df.write.mode(\"overwrite\").csv(\"C:/Users/ADIPAKSH/Desktop/Pyspark/output8/weekly_sales_by_year_month_date.csv\")\n",
    "\n",
    "# Step 8: Write as Parquet and ORC\n",
    "result_df.write.mode(\"overwrite\").parquet(\"C:/Users/ADIPAKSH/Desktop/Pyspark/output8/weekly_sales_by_year_month_date_parquet\")\n",
    "result_df.write.mode(\"overwrite\").orc(\"C:/Users/ADIPAKSH/Desktop/Pyspark/output8/weekly_sales_by_year_month_date_orc\")\n",
    "\n",
    "# Step 9: Partition by Year and Month\n",
    "result_df.write.partitionBy(\"Year\", \"Month\").mode(\"overwrite\").parquet(\"C:/Users/ADIPAKSH/Desktop/Pyspark/output8/partitioned_by_year_month\")\n",
    "\n",
    "\n",
    "# Optional: Stop Spark session\n",
    "# spark.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7a94018-0b7c-45cf-8578-d77eae2c76cb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
